### Os comandos a saber
##

Além da interface do usuário, há outro componente que você pode usar para interagir com o Airflow, que é a interface de linha de comando.

E a interface de linha de comando é extremamente útil, pois você pode executar alguns comandos que não estão realmente disponíveis na interface do usuário.

E mais importante, se você não conseguir acessar a interface do usuário por alguns motivos, ainda poderá interagir com o Airflow por meio da interface de linha de comando.

Obviamente, não veremos todos os comandos disponíveis.

Caso contrário, você simplesmente adormecerá e eu não quero fazer isso de jeito nenhum.

Então vamos focar apenas nos comandos que eu acho que são os mais importantes.

Então, sem mais delongas, vamos começar.

Primeiro, como usamos o Astronomer CLI, precisamos nos conectar a um dos contêineres docker correspondentes à nossa instância do Airflow para acessar a interface de linha de comando do Airflow.

Para fazer isso, digite ```docker ps``` então ```docker exec -it conteiner-num /bin/bash``` Vamos usar, por exemplo, o servidor web e slash bin slash bash para abrir uma estação bash interativa nesse contêiner.

Pressione enter e, como você pode ver, estamos dentro do contêiner docker do servidor web.

Então, a partir daí, podemos interagir com o Airflow.

O primeiro comando que você executará assim que instalar o Airflow pela primeira vez é ```airflow db init```.

Airflow DB init permite inicializar o banco de dados do Airflow, e você deve executar esse comando primeiro para não apenas inicializar o banco de dados, mas também gerar os arquivos e pastas necessários para o Airflow.

Depois de executar o init do Airflow DB, outro comando que você pode precisar é o ```airflow db upgrade```.

E como você pode imaginar, este comando permite que você atualize sua instância do Airflow.

Portanto, se houver uma nova versão do Airflow, você precisará executar esse comando para atualizar o esquema do meta banco de dados.

Então você redefiniu o ```airflow db reset``` e tenho certeza de que nunca usará esse comando se tiver o Airflow em produção.

Caso contrário, você pode usar esse para fazer experimentos, pois permite remover tudo no banco de dados.

Então, basicamente, é exatamente como se você fosse começar do zero.

Portanto, não use esse comando na produção.

Caso contrário, você terá muitos problemas, eu acho.

Tudo bem.

E os outros comandos? Bem, obviamente existe o comando ```airflow web server``` para iniciar o servidor web, portanto, a interface do usuário.

Temos o agendador Airflow ```airflow scheduler``` para iniciar o agendador e temos o trabalhador ```airflow celery worker``` para iniciar um trabalhador de aipo.

Lembre-se, vimos rapidamente que, com o aipo, você pode executar tarefas em várias máquinas.

Bem, nessas várias máquinas, você terá que executar o comando Airflow celery worker para indicar que uma determinada máquina pode ser usada pelo Airflow para executar tarefas.

Então, esse é o objetivo do trabalhador de aipo do Airflow.

Então, temos alguns outros comandos que são super importantes para interagir com seus DAGs ou suas tarefas.

Vamos começar com ```airflow dags pause``` ou ```airflow dags unpause```.

Esses comandos são exatamente o que você pode fazer com a alternância.

Lembre-se, sempre que quiser acionar um DAG, você precisa ativar a alternância desse DAG na interface do usuário.

Bem, adivinhe? Na interface de comando, você também pode fazer isso com a pose dos DAGs do Airflow ou com a desposição dos DAGs do Airflow se quiser começar a agendar seu DAG.

Em seguida, há o comando ```airflow dags trigger```, para acionar um DAG.

Portanto, se você não tiver acesso à interface do usuário e quiser acionar um DAG, poderá fazer isso com esse comando.

Além disso, você pode especificar uma data de execução, uma data de execução específica, se necessário.

Agora, digamos que você queira listar todos os DAGs que possui em sua instância do Airflow.

Para fazer isso, podemos digitar ```airflow dags list``` Se você apertar enter, como você pode ver, temos apenas um DAG que é example_DAG, bem como o caminho do arquivo, o proprietário e se ele foi colocado ou não.

Este comando é útil para saber se o Airflow está ciente do seu novo arquivo DAG, por exemplo.

Em seguida, você pode listar as tarefas de um determinado DAG, então vamos colocar ```airflow tasks list (name_dag)``` e então você precisa especificar o ID DAG.

Nesse caso, por exemplo, DAG.

Aperte enter.

E como você pode ver, obtemos a lista de todas as tarefas que este DAG possui.

Novamente, se você adicionou uma nova tarefa em seu arquivo DAG e deseja ter certeza de que o Airflow está ciente dessa nova tarefa, você pode usar esse comando para fazer isso.

Ok, agora há mais dois comandos sobre os quais quero falar.

O primeiro é o teste de tarefas do Airflow ```airflow tasks test```, e esse comando é extremamente, extremamente útil.

Por que? Porque você executará esse comando sempre que adicionar uma nova tarefa em seu DAG.

De fato, este comando permite executar uma tarefa específica sem verificar as dependências, [inaudível 00:04:53] o estado dessa tarefa no meta banco de dados.

E você deve sempre verificar suas tarefas com esse comando para garantir que, antes de enviar qualquer DAG na produção, suas tarefas realmente funcionem.

Portanto, como prática recomendada, sempre use esse comando sempre que adicionar uma nova tarefa em seu DAG.

Se você está se perguntando como essa vírguland funciona, bem, bem simples.

Você só precisa digitar ```airflow tasks test``` em seguida, um ID DAG.

Vamos colocar ```example_DAG,``` em seguida, um ID de tarefa.

Digamos ```bash_print _date_1``` e por último, mas não menos importante, precisamos especificar uma data de execução no passado.

Digamos ```2021-01-01``` E é isso.  

Comando completo exemplo: ```airflow tasks test example_DAG bash_print _date_1 2021-01-01```

Se você apertar enter e esperar um pouco, como você pode ver, obtemos alguns logs e logo na parte inferior dos logs, você verá, em alguns segundos, que esse comando foi bem-sucedido.

Isso é o que você pode ver ali.

Novamente, sempre execute esse comando para garantir que sua tarefa funcione.

Finalmente, há um último comando.

```airflow dags backfill``` Ao executar ```airflow dags backfill`` você pode executar novamente execuções DAG anteriores.

Por exemplo, você posou seu DAG durante cinco dias e deseja executar todas as execuções de DAG não acionadas.

Bem, você pode fazer isso com esse comando.

As únicas opções que você tem que colocar são ```-s``` para a data de início.

Digamos ```2021-01-01,``` em seguida, uma data final, ```-e``` ```21-01-05``` E, finalmente, você pode indicar se deseja redefinir as execuções DAG já acionadas anteriormente.

Com a opção ```--reset_dagruns``` Em seguida, o DAG ID do DAG que você deseja preencher.

Digamos exemplo, ```_dag``` Ok, lembre-se de que esse comando é útil sempre que você deseja executar novamente as execuções anteriores do DAG.

Exemplo: ```airflow dags backfill -s 2021-01-01 -e 2021-01-05 --reset_dagruns exemple_dag```

Então, esses são os comandos mais importantes na minha opinião que você precisa saber para começar a usar o Airflow da melhor maneira possível.

Há uma última maneira de interagir com o Airflow.

Qual é?


##
### The Commands to Know
##
In addition to the user interface, there is another component you can use to interact with Airflow, which is the command line interface. And the command line interface is extremely useful, as you can execute some commands that are not really available on the UI. And more importantly, if you can't access the UI for some reasons, you can still interact with Airflow through the command line interface. Obviously, we are not going to see all of the commands available. Otherwise, you will just fall asleep and I don't want to do that at all. So we are going to focus only on the commands that I think are the most important one. So without further ado, let's get started. First, as we use the Astronomer CLI, we need to connect into one of the docker containers corresponding to our Airflow instance in order to access the command line interface of Airflow.

To do this, type \&quot;docker PS,\&quot; then \&quot;Docker exec-it.\&quot; Let's use, for example, the web server and slash bin slash bash in order to open an interactive bash station against that container. Hit enter, and as you can see, we are inside the docker container of the web server. Then, from there, we can interact with Airflow. The first command you will execute as soon as you install Airflow for the first time is Airflow db init. Airflow DB init allows you to initialize database of Airflow, and you have to execute that command first in order to not only initialize the database, but also generate the files and folders needed by Airflow. Once you have executed Airflow DB init, another command you might need is Airflow DB upgrade. And as you can guess, this command allows you to upgrade your Airflow instance. So if there is a new version of Airflow, you need to execute that command in order to upgrade the schema of the meta database.

Then you have Airflow DB reset, and I'm pretty sure that you will never use that command if you have Airflow in production. Otherwise, you can use that one for making experiments, as it allows you to remove everything in the database. So basically, it's exactly like you will start from scratch. So don't use that command in production. Otherwise, you will end up with a lot of troubles, I think. All right. So what about the other commands? Well, there is obviously the command Airflow web server in order to start the web server, so the user interface. We have Airflow scheduler in order to start the scheduler and we have Airflow celery worker in order to start a celery worker. Remember, we have quickly seen that with celery, you are able to execute tasks on multiple machines. Well, on those multiple machines, you will have to execute the command Airflow celery worker in order to indicate that a given machine can be used by Airflow to execute tasks.

So that's the purpose of Airflow celery worker. Then, we have some other commands that are super important to interact with your DAGs or your tasks. Let's begin with Airflow dags pose or Airflow dags unpose. Those commands are exactly what you can do with the toggle. Remember, whenever you want to trigger a DAG, you need to turn on the toggle of that DAG on the UI. Well, guess what? From the command interface, you can do that as well with Airflow DAGs pose or Airflow DAGs unpose if you want to start scheduling your DAG. Next, there is the command Airflow DAGs trigger, in order to trigger a DAG. So if you don't have access to the user interface and you want to trigger a DAG, you can do that with that command. Also, you can specify an execution date, a specific execution date, if needed. Now, let's say you want to list all the DAGs you have in your Airflow instance. To do that, we can type \&quot;Airflow DAGs list.\&quot; If you hit enter, as you can see, we only got one DAG which is example_DAG as well as the 
file path, the owner, and if it is posed or not. This command is useful to know if Airflow is aware of your new DAG file, for example.

Then you can list the tasks of a given DAG, so let's put \&quot;Airflow tasks list,\&quot; and then you need to specify the DAG ID. In that case, example, DAG. Hit enter. And as you can see, we obtain the list of all tasks that this DAG has. Again, if you have added a new task in your DAG file and want to be sure that Airflow is aware of this new task, you can use that command to do that. Okay, now there are two more commands that I absolutely want to talk about. The first one is Airflow tasks test, and this command is extremely, extremely useful. Why? Because you will execute that command each time you add a new task in your DAG. Indeed, this command allows you to execute a specific task without checking for the dependencies, [inaudible 00:04:53] the state of that task in the meta database.

And you should always check your tasks with that command to make sure that before pushing any DAG in production, actually, your tasks work. So as a best practice, always use that command each time you add a new task in your DAG. If you're wondering how this command works, well, pretty simple. You just need to type \&quot;Airflow tasks test,\&quot; then a DAG ID. Let's put \&quot;example_DAG,\&quot; then a task ID. Let's say \&quot;bash print date one,\&quot; and last but not least, we need to specify an execution date in the past. Let's say \&quot;2021-01-01.\&quot; And that's it. If you hit enter and wait a little bit, as you can see, we get some logs and right at the bottom of the logs, you will see, in a few seconds, that this command success. That's what you can see right there. Again, always execute that command to make sure that your task works.

Finally, there is one last command. \&quot;Airflow DAGs backfill.\&quot; By executing \&quot;Airflow DAGs backfill,\&quot; you can rerun past DAG runs. For example, you have posed your DAG during five days, and you want to run all the nontriggered DAG runs. Well, you can do that with that command. The only options you have to put are \&quot;-S\&quot; for the start date. Let's say \&quot;2021-01-01,\&quot; then an end date, \&quot;-E,\&quot; \&quot;21-01-05.\&quot; And finally, you can indicate if you want to reset the past already triggered DAG runs. With the option \&quot;--reset_dagruns.\&quot; Then the DAG ID of the DAG you want to backfill. Let's say example, \&quot;_dag.\&quot; Okay, so remember that this command is useful whenever you want to rerun past DAG runs. So that's the most important commands in my opinion that you have to know in order to start using Airflow in the best possible way. There is one last way to interact with Airflow. Which one is it?

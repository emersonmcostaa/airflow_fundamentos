### O que é Airflow?
##

O que é Airflow? Quais são os benefícios de usar o Airflow? E qual Airflow não é? É exatamente isso que vamos descobrir nesse vídeo. Vamos começar com a definição formal do Apache Airflow. O Apache Airflow é uma plataforma de código aberto para criar, agendar e monitorar fluxos de trabalho de forma programática. Em outras palavras, o Apache Airflow é um orquestrador para criar pipelines de dados dinâmicos e executar tarefas na ordem certa, da maneira certa, no momento certo. Por exemplo, você deseja produzir estatísticas com base nos dados do usuário. Como você pode fazer isso? Criando um pipeline de dados com duas tarefas. A primeira tarefa é responsável pelo processamento dos dados, você pode acionar um trabalho do Spark a partir do Airflow e, em seguida, deseja armazenar essas estatísticas em um banco de dados. Novamente, você pode fazer isso em uma tarefa com o Airflow. Portanto, se você tiver várias tarefas que deseja executar em uma determinada ordem em que essas tarefas estão interagindo com outras ferramentas, como Spark, Postgres, MySQL, dbt, elasticsearch, você pode fazer isso com o Airflow. E mais importante, você será avisado se houver alguma falha em seus pipelines de dados. 

Tudo bem, sabemos o que é Airflow. Vamos descobrir seus benefícios. E o primeiro benefício do Airflow é que os pipelines de dados são dinâmicos. Como eles são codificados em Python, qualquer coisa que você possa fazer em Python, poderá fazê-lo em seus pipelines de dados. O que você pode fazer com seus pipelines de dados é bastante ilimitado. Então, o Airflow é altamente escalável, o que significa que você pode executar quantas tarefas quiser. Por exemplo, você tem o cluster Kubernetes, deseja executar suas tarefas nesse cluster Kubernetes, pode fazer isso escolhendo o executor certo, que é o executor Kubernetes. Você tem um cluster de servidores com várias máquinas para executar suas tarefas, você pode fazer isso com o Airflow, escolhendo o executor Siri. De acordo com a arquitetura que você possui, você tem uma forma de executar suas tarefas. O benefício definido do Airflow é que ele é altamente interativo. 

Você tem três maneiras de interagir com o Airflow. A primeira é a interface do usuário, e a interface do usuário é linda. Você vai gostar de monitorar seus pipelines de dados e tarefas na interface do usuário do Airflow. A segunda maneira de interagir com o Airflow é a interface de linha de comando. Se você deseja executar alguns comandos e não gosta da interface do usuário, pode fazer isso escolhendo a interface de linha de comando. E por último, mas não menos importante, se você tiver um front-end e quiser executar um pipeline de dados específico quando um botão for clicado, poderá fazer isso por meio da API REST. 

O último benefício sobre o qual gostaria de falar é que o Airflow é altamente extensível. Você pode personalizar o Airflow tanto quanto precisar. Por exemplo, há uma nova ferramenta e você deseja interagir com essa nova ferramenta. Você não precisa esperar que a comunidade ou alguém crie um plugin para você. Você pode criar seu próprio plug-in, adicioná-lo ao Airflow e interagir com essa nova ferramenta. Você deseja modificar a interface do usuário, você pode fazer isso. Você quer mudar a forma como suas tarefas são executadas, você pode fazer isso. Você deseja adicionar algumas novas funcionalidades no Airflow, você pode fazer isso. Novamente, você pode personalizar o Airflow o quanto precisar. 

Ok, neste ponto, sabemos o que é o Airflow e quais são os benefícios de usá-lo. Mas não vimos o que o Airflow não é. Airflow não é uma solução de streaming, nem uma estrutura de processamento de dados. E isso é superimportante. Não use o Airflow como o Spark. Não tente processar terabytes de dados no Airflow, você acabará com um estreito. O Airflow não pretende ser uma estrutura de processamento de dados como o Spark, é um orquestrador, é o melhor orquestrador. Portanto, se você tiver terabytes de dados para processar, acione um trabalho do Spark no Airflow e deixe o Spark fazer o trabalho. Este não é o trabalho do Airflow. Finalmente, o Airflow não é uma solução de streaming, então não tente enganar o [inaudível 00:03:55], para acionar seu pipeline de dados a cada segundo, não funcionará. Mantenha no Airflow o melhor orquestrador, permitindo criar pipelines de dados dinâmicos e executar tarefas da maneira certa, na ordem certa e na hora certa. Ah, e a propósito, para executar suas tarefas, o Airflow precisa de alguns componentes principais. Você conhece eles?

##
### What is Airflow?
##

What is airflow? What are the benefits of using airflow ? And what airflow is not? That's exactly what we are going to discover in that video. Let's begin with the formal definition of Apache Airflow. Apache Airflow is an open source platform to programmatically author, schedule and monitor workflows. In other words, Apache Airflow is an orchestrator for creating dynamic data pipelines and executing tasks in the right order, in the right way, at the right time. For example, you want to produce statistics based on user data. How can you do that? By creating a data pipeline with two tasks. The first task is in charge of processing the data, you could trigger a Spark job from airflow, and then you want to store those statistics into a database. Again, you can do that in a task with Airflow. So if you have multiple tasks that you want to execute in a given order where those tasks are interacting with other tools, such as Spark, Postgres, MySQL, dbt, elasticsearch, you can do that with Airflow. And more importantly, you would be warned if there is any failure in your data pipelines.

All right, we know what is Airflow. Let's discover its benefits. And the first benefit of Airflow is that data pipelines are dynamic. Since, they are coded in Python, anything you can do in Python, you can do it in your data pipelines. What you can do with your data pipelines is quite limitless. Then, Airflow is highly scalable, which means you can execute as many tasks as you want. For example, you have the Kubernetes cluster, you want to execute your tasks on that Kubernetes cluster, you can do that by choosing the right executor, which is the Kubernetes executor. You have a server cluster with multiple machines to execute your tasks, you can do that with Airflow, by choosing the Siri executor. According to the architecture you have, you have a way to execute your tasks. The set benefit of Airflow is that it is highly interactive.

You have three ways of interacting with Airflow. The first one is, the user interface, and the user interface is beautiful. You will enjoy monitoring your data pipelines and tasks on the user interface of Airflow. The second way of interacting with Airflow is, the command line interface. If you want to execute some commands and you don't like the UI, you can do that choosing the command line interface. And last but not least, if you have a front-end and you want to execute a specific data pipeline when a button is clicked, you can do that through the REST API.

The last benefit I would like to talk about is that, Airflow is highly extensible. You can customize Airflow as much as you need. For example, there is a new tool and you want to interact with that new tool. You don't need to wait for the community or anybody to create a plugin for you. You can create your own plugin, add it to Airflow, and then interact with that new tool. You want to modify the user interface, you can do that. You want to change the way your tasks are executed, you can do that. You want to add some new functionalities in Airflow, you can do that. Again, you can customize Airflow as much as you need.

Okay, at this point, we know what is Airflow and what are the benefits of using it. But we didn't see what Airflow is not. Airflow is not a streaming solution, neither a data processing framework. And this is super important. Don't use Airflow like Spark. Don't try to process terabytes of data in Airflow, you will end up with a narrow. Airflow is not meant to be a data processing framework like Spark, it is an orchestrator, it is the best orchestrator. So if you have terabytes of data to process, trigger a Spark job from Airflow, and then let Spark do the job. This is not the job of Airflow. Finally, Airflow is not a streaming solution, so don't try to trick the [inaudible 00:03:55] at all, in order to trigger your data pipeline every second, it won't work. Keep in Airflow the best orchestrator, allowing you to create dynamic data pipelines and execute tasks in the right way, in the right order at the right time. Oh, and by the way, in order to execute your tasks, Airflow needs some core components. Do you know them?
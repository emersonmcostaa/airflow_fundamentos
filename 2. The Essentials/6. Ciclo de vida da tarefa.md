### Ciclo de vida da tarefa
##

Hora de descobrir o ciclo de vida de uma tarefa.

O que acontece quando uma tarefa está pronta para ser acionada no Airflow? 

E para ilustrar isso, fiz um lindo esquema. 

O que você pode ver aqui são os diferentes componentes do Airflow. 

Temos o servidor web, servindo a interface do usuário. 

O Metastore, onde todos os metadados relacionados ao Airflow, seus DAGs, suas tarefas são armazenados. 

Depois temos o agendador, responsável por agendar suas tarefas. 

O executor, responsável por definir como suas tarefas serão executadas, e por último, mas não menos importante, a pasta Dags onde ficarão seus DAGs.

Qual é o processo típico pelo qual você passará para executar suas primeiras tarefas no Airflow? 

Bem, primeiro você precisa criar um novo arquivo Python. 

Vamos chamá-lo de ```dag.py```, onde seu pipeline de dados é definido. 

Este arquivo deve estar na pasta Dags. 

É aqui que todos os seus arquivos correspondentes aos seus pipelines de dados estarão.

Depois de adicionar o arquivo ```dag.py```, correspondente ao seu pipeline de dados, esse arquivo será analisado, tanto pelo servidor web quanto pelo agendador, e você deve ter em mente uma coisa importante aqui, o servidor web analisa DAGs a cada 30 segundos por padrão, enquanto os agendamentos analisam para novos DAGs, para novos arquivos a cada cinco minutos por padrão.

Ok, lembre-se de que, se você não vir seu DAG aparecendo na interface do usuário imediatamente, é porque precisa aguardar 30 segundos por padrão. 

Além disso, o agendador analisa novos arquivos DAG a cada cinco minutos por padrão. 

Portanto, tenha em mente essa diferença. 

É extremamente importante. 

Depois que o DAG tiver sido analisado pelo servidor Web e pelo agendador, se um DAG estiver pronto para ser acionado, bem, o agendador criará um objeto DagRun e lembre-se de que um DagRun é uma instância do seu DAG.

Então você obtém um objeto DagRun com o status em execução, no Metastore.

No início, as tarefas em um objeto DagRun não têm status. 

Então, assim que a tarefa estiver pronta para ser acionada, um objeto de instância de tarefa é criado pelo agendador e esse objeto de instância de tarefa, ou sua tarefa tem o status, agendado. 

Em seguida, o agendador envia a tarefa para o executor onde, desta vez, a tarefa está com o status na fila. 

Uma vez que a tarefa tenha a estátua enfileirada, o executor está pronto para pegar essa tarefa e executá-la em um trabalhador. 

E é aí que a tarefa está em execução. 

Agora, a tarefa está executando as atualizações do executor, o status do objeto de instância da tarefa no Metastore com o status, executando também. 

E assim que a tarefa é concluída, o status dessa tarefa é sucesso, se tudo funcionou conforme o esperado, e o status do objeto de instância da tarefa é modificado pelo executor também para sucesso no Metastore.

Agora todas as tarefas estão concluídas. 

O agendador verifica se o trabalho foi concluído, se não há mais tarefas para agendar, caso contrário, o objeto DagRun é marcado com o status, sucesso, se não há falha em suas tarefas e, em seguida, o servidor web atualiza a interface do usuário, ou, na verdade, sempre que você atualizar a página, verá se suas tarefas foram concluídas ou não.

Essa é a jornada típica de uma tarefa no Airflow. 

Portanto, lembre-se de que, no início, uma tarefa não tem status; depois, quando essa tarefa está pronta para ser acionada, a tarefa tem o status agendada. 

Então, uma vez que a tarefa é enviada para o executor pelo agendador, esta tarefa tem o status na fila. 

Em seguida, o trabalhador executa a tarefa. 

Assim, o status da tarefa torna-se em execução e, finalmente, se você não obteve nenhum erro, a tarefa será marcada com o status sucesso.


### Task Lifecycle
##

Time to discover the lifecycle of a task. What happens when a task is ready to be triggered in Airflow? And to illustrate this, I made a beautiful schema. What you can see here are the different components of Airflow. We have the web server, serving the UI. The Metastore, where all the metadata related to Airflow, your DAGs, your tasks are stored. Then we have the scheduler, in charge of scheduling your tasks. The executor, in charge of defining how your tasks are going to be executed, and last but not the least, the folder Dags where your DAGs will be.

What is the typical process you will go through in order to run your first tasks in Airflow? Well, first you have to create a new Python file. Let's name it dag.py, where your data pipeline is defined. This file should be into the folder Dags. This is where all of your files corresponding to your data pipelines will be. Once you have added the file dag.py, corresponding to your data pipeline, this file is going to be parsed, both by the web server and the scheduler, and you have to keep in mind one important thing here, the web server parses DAGs every 30 seconds by default, whereas the schedules parses for new DAGs, for new files every five minutes by default.

Okay, so keep in mind that if you don't see your DAG showing up on the UI right away, that's because you have to wait 30 seconds by default. Also the scheduler parses for new DAG files every five minutes by default. So really keep in mind this difference. It is extremely important. Once the DAG has been parsed by the web server and the scheduler, if a DAG is ready to be triggered, well, the scheduler creates a DagRun object, and remember that a DagRun is an instance of your DAG.

So you get a DagRun object with the status running, in the Metastore. At the beginning tasks in a DagRun object have no status. Then as soon as the task is ready to be triggered, a task instance object is created by the scheduler and this task instance object, or your task has the status, scheduled. Then the scheduler sends the task into the executor where, this time the task has the status queued. Once the task has the statue queued, the executor is ready to take that task and execute it into a worker. And that's when the task is running. Now, the task is running the executor updates, the status of the task instance object in the Metastore with the status, running, as well. And as soon as the task is done, the status of that task is success, if everything have worked as expected, and the status of the task instance object is modified by the executor as well to success in the Metastore.

Now all the tasks are completed. The scheduler checks, if the work is done, if there is no more tasks to schedule, if not, the DagRun object is marked with the status, success, if there is no failure in your tasks and then the web server updates the UI, or actually whenever you refresh the page, you will see if your tasks are done or not. That's the typical journey of a task in Airflow. So keep in mind that at the beginning, a task has no status, then when that task is ready to be triggered, the task has the status, scheduled. Then once the task is pushed into the executor by the scheduler, this task has the status queued. Then the worker execute the task. So the status of the task becomes running, and finally, if you didn't get any error the task will be marked with the status success.

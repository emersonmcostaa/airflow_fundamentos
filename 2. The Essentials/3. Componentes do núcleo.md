### Componentes do núcleo
##

Por padrão, o Airflow executa três componentes principais. 

Vamos descobrir quais. 

E o primeiro componente principal do Airflow é o servidor da Web.

O Web Server é um servidor flask com gunicorn servindo a interface do usuário. 

Portanto, sem o servidor da Web, você não pode acessar a interface do usuário do Airflow e, portanto, não é realmente capaz de monitorar seu aplicativo de dados da melhor maneira possível.

Então, o segundo componente principal do Airflow é o Agendador. 

O Agendador é o coração do Airflow. 

Você tem que cuidar do seu Agendador. 

Por que? Porque, sem o Agendador, você não consegue agendar, não consegue acionar nenhuma tarefa. 

Se o Agendador ficar inativo, você não poderá acionar nenhuma tarefa em sua instância do Airflow. 

Aliás, no Airflow 2.2, você tem a possibilidade de ter vários Agendadores rodando ao mesmo tempo. 

Portanto, se um Agendador for concluído, você ainda terá os outros em execução e ainda poderá acionar tarefas em sua instância do Airflow. 

Isso é muito importante.

Por fim, o último componente principal do Airflow é o banco de dados de metadados. 

Todos os metadados relacionados a usuários, trabalhos, variáveis, conexões, quaisquer dados relacionados ao Airflow são armazenados no banco de dados de metadados. 

E, basicamente, qualquer banco de dados compatível com SQLAlchemy pode ser usado para o Airflow. 

Por exemplo, Postgres, MySQL, Oracle, SQLite, até MongoDB, mesmo que não seja muito recomendado, você pode usar o MongoDB, mas com limitações. 

Por exemplo, com o Airflow 2.2, você não pode ter vários Agendadores em execução ao mesmo tempo, se usar o MongoDB. 

Então, esses são os três componentes principais do Airflow: o servidor da Web, o agendador e o meta banco de dados.

Agora tem mais dois componentes que eu gostaria de falar porque são super importantes.

E o primeiro é o Executor. 

O Executor define como suas tarefas serão executadas. 

E repito, o Executor define como você ou as tarefas serão executadas pelo Airflow. 

Por exemplo, você tem um cluster Kubernetes, deseja executar suas tarefas no Kubernetes, usará o KubernetesExecutor. 

Você tem um cluster Celery com várias máquinas e deseja usá-lo para executar suas tarefas, você usará o CeleryExecutor. 

Se você tem uma máquina muito poderosa e deseja executar várias tarefas ao mesmo tempo nela, pode fazer isso com o LocalExecutor. 

E por padrão, o Airflow possui o SequentialExecutor, o que significa que suas tarefas são executadas sequencialmente. 

Um após o outro. 

Lembre-se de que o Executor define como suas tarefas são executadas, em qual sistema e há uma deixa atrás de cada executor para ordenar suas tarefas. 

Agora sabemos que com o Executor definimos como as tarefas são executadas.

O Worker é onde sua tarefa é realmente executada. 

O trabalhador é um processo ou um subprocesso onde sua tarefa é executada. 

Por exemplo, se você usar o Kubernetes, terá um pod com um processo dentro do pod onde sua tarefa é executada. 

O mesmo para o Celery, o mesmo para o LocalExecutor. 

Com o LocalExecutor, você terá vários subprocessos onde suas tarefas são executadas, e esses subprocessos ou processos são Workers. 

Lembre-se disso.

Resumindo, temos três componentes principais: o Web Server, o Scheduler e o Meta Database, e dois componentes adicionais que ainda estão rodando nos bastidores, 

que são o Executor, para definir como suas tarefas são executadas, 

e o Worker , onde suas tarefas são executadas.

Tudo bem. 

É ótimo conhecer os principais componentes do Airflow, mas a próxima pergunta é como eles funcionam juntos? 

É exatamente isso que vamos descobrir no próximo vídeo.

##
### Core Components
##

By default, Airflow runs three core components. Let's find out which ones. And the first core component of Airflow is the Web Server. The Web Server is a flask server with gunicorn serving the user interface. So without the Web Server, you can't access the user interface of Airflow, and so you are not really able to monitor your data app appliance in best possible way.

Then, the second core component of Airflow is the Scheduler. The Scheduler is the heart of Airflow. You have to take care of your Scheduler. Why? Because, without the Scheduler, you are not able to schedule, you are not able to trigger any tasks. If the Scheduler goes down, you won't be able to trigger any tasks in your Airflow instance. And by the way, in Airflow 2.2, you have the possibility to have multiple Schedulers running at the same time. So if one Scheduler goes done, you still have the others running and so you can still trigger tasks in your Airflow instance. This is super important.

Finally, the last core component of Airflow is the Metadata Database. All Metadata related to users, jobs, variables, connections, any data related to Airflow is stored into the Metadata Database. And, basically, any database that is compatible with SQLAlchemy can be used for Airflow. For example, Postgres, MySQL, Oracle, SQLite, even MongoDB, even if it's not really recommended, you can use MongoDB, but with limitations. For example, with Airflow 2.2, you are not able to have multiple 

Schedulers running at the same time, if you use MongoDB. So, that's the three core components of Airflow: the Web Server, the Scheduler and the Meta Database.
Now there are two more components that I would like to talk about because they are super important. And the first one is the Executor. The Executor defines how your tasks are going to be executed. And I repeat, the Executor defines how you or tasks are going to be executed by Airflow. For example, you have a Kubernetes cluster, you want to execute your tasks on Kubernetes, you will use the KubernetesExecutor. You have a Celery cluster with multiple machines and you want to use it to execute your tasks, you will use the CeleryExecutor. If you have a very powerful machine and you want to execute multiple tasks at the same time on it, you can do that with the LocalExecutor. And by default, Airflow has the SequentialExecutor, which means your tasks are executed sequentially. One, after the other.Keep in mind that the Executor defines how your tasks are executed, on which system, and there is a cue behind each executor to order your tasks. Now we know that with the Executor we define how tasks are executed.

The Worker is where your task is actually executed. The worker is a process or a sub-process where your task is executed. For example, if you use Kubernetes, you will have a pod with a process inside the pod where your task is executed. Same for Celery, same for the LocalExecutor. With the LocalExecutor, you will have multiple sub-processes where your tasks are executed, and those sub-processes or processes are Workers. Keep in mind this.

To sum up, we have three core components: the Web Server, the Scheduler, and the Meta Database, and two additional components that are still running behind the scene, which are the Executor, to define how your tasks are executed, and the Worker, where your tasks are executed.

All right. It's great to know the core components of Airflow, but the next question is how do they work together? That's exactly what we are going to discover in the next video.


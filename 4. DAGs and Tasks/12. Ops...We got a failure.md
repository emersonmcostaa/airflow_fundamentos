###
##

Ops... Ocorreu uma falha

Neste ponto, criamos com sucesso um pipeline de dados para descobrir todos os conceitos que você pode ter durante o exame. Mas ainda falta algo. De fato, e se tivermos uma tarefa em fracasso? O que deveríamos fazer? Como podemos monitorar isso? É isso que vamos descobrir nesse vídeo.
Então, vamos criar um erro falso. Ao DAG. Ao processar dados com o BashOperator, ao invés de ```'exit 0'```, colocamos ```'exit 1'```

E ao fazer isso, essa tarefa terminará em fracasso. Salve o arquivo e vamos para a interface do usuário. A partir daí começamos a agendar o pipeline de dados, atualize a página. Como você pode ver, temos uma execução DAG em execução. Vamos atualizar novamente e temos um problema. Se clicarmos no DAG na exibição do gráfico, como você pode ver, o processamento de dados está pronto para uma nova tentativa.

Então o que isso significa? Isso significa que esta tarefa teve uma falha e, portanto, o Airflow tentará executá-la novamente automaticamente. E, de fato, se você entrar no código ali nos argumentos padrão, especificamos que cada vez que uma tarefa falha, a tarefa deve ser repetida cinco vezes antes de terminar com o uso do estado falhou. E entre cada tentativa, temos um atraso de tempo delta de cinco minutos. Então, entre cada tentativa, temos cinco minutos. Isso é o que você pode ver aqui.
Então, o que devemos fazer sempre que temos uma tarefa em falha? Bem, você deve clicar na tarefa, ir para o log e, a partir daí, tentar depurar o que está acontecendo com sua tarefa. Como você pode ver aqui, o comando retornou um código de saída diferente de zero. Então, se quisermos corrigir esse problema, só precisamos voltar para a tarefa ali mesmo e colocar zero em vez de um, salvar o arquivo, voltar para a interface do usuário, clicar na visualização do gráfico, depois na tarefa e clicar em limpar.

Limpar permite que você repita a tarefa. Clique, ok. Agora a cor da borda é branca. E se esperarmos um pouco, o processamento dos dados foi concluído com sucesso. Agora, e se você tiver várias execuções de DAG com várias falhas? Deixe-me mostrar-lhe isto. Pause o DAG. Vamos navegar, executar o DAG e excluir a execução do DAG para limpar tudo. O mesmo para as instâncias de tarefa. Selecione todos eles, ações e claro.
E digamos que, em vez de definir os parâmetros de recuperação como false, defina-os como true para que acabemos com três execuções de DAG. Por fim, no operador batch, coloque um para criar uma falha, salve o arquivo, volte para a UI, clique em DAGs e vamos começar a agendar o DAG novamente. Recarregue a página. Como você pode ver desta vez, temos três execuções de DAG em execução, mas também temos três tarefas para nova tentativa.

De fato, se clicarmos no DAG na visualização em árvore, que é a melhor visualização se você quiser dar uma olhada em todas as suas execuções de DAG. Temos três execuções de DAG e três tarefas para nova tentativa. Ok, aqui, a maneira ingênua de corrigir sua tarefa é clicar na tarefa e, em seguida, clicar em limpar para tentar novamente. Mas se você tiver centenas de execuções de DAG, definitivamente não deseja fazer isso. Vai ser muito chato e demorado. Portanto, em vez de fazer isso, vá para navegar, instâncias de tarefas e clique em pesquisar, adicionar filtro e estado. A partir daí, procure o estado up_for_retry, aperte enter e você obterá apenas as tarefas que lhe interessam. E se você clicar em pesquisar e adicionar um segundo filtro, poderá até filtrar pelo DAG ID.

Portanto, se você clicar no ID DAG e adicionar simple_dag, certifique-se de obter apenas as instâncias de tarefas correspondentes ao DAG correto. Assim que tiver isso, vá para o código, corrija o problema. Então, vamos colocar zero aqui, salvar o arquivo e selecionar todas as tarefas, clicar em ações e limpar. Então, basicamente, vamos tentar novamente todas essas tarefas de uma vez, em vez de clicar na tarefa, clicar em limpar e assim por diante para cada diagrama.

Então clique em, ok, volte para DAGs e pronto. Isso é o que você pode ver na visualização em árvore. Portanto, esta é a maneira mais fácil de repetir várias tarefas ao mesmo tempo. E, novamente, você acabará com muitas execuções de DAG e, às vezes, com muitas falhas no mesmo ponto. É por isso que realmente aconselho você a usar a navegação, as instâncias de tarefas e os filtros para tentar novamente suas tarefas depois de corrigir o problema.

E mais uma coisa, evite repetir a execução do DAG porque se você clicar no segundo logo ali e clicar em limpar, todas as tarefas serão repetidas. Mas, na verdade, se apenas uma tarefa falhou bem, novamente, você está desperdiçando seus recursos e seu tempo. Portanto, tente ser específico sempre que houver uma falha e repita apenas a tarefa com falha, não toda a execução do DAG.

Tudo bem, agora eu gostaria de mostrar mais duas coisas. Digamos que você queira ser avisado se houver uma falha ou se houver uma nova tentativa. Como você pode fazer isso? Bem, existem dois parâmetros para fazer isso. Nos argumentos padrão, se você especificar o parâmetro ```email_on_failure = true```, receberá um email assim que a tarefa estiver em falha. Se você deseja receber um e-mail assim que a tarefa for repetida, você pode colocar ```email_on_retry = true``` e, novamente, receberá um e-mail assim que a tarefa for refeitatestado.

Por último, mas não menos importante, você deve especificar um e-mail se quiser que esses parâmetros funcionem. Então, vamos colocar um ```'e-mail' : 'admin@astro.io' ```e, finalmente, você deve configurar o servidor SMTP, caso contrário, não poderá enviar nenhum e-mail do seu ar, por exemplo. Portanto, essa é uma maneira de ser avisado se houver uma falha ou uma nova tentativa no seu DAG.

Agora, o que mais? Digamos que você queira executar algo se uma tarefa falhar. Como você pode fazer isso? Usando o parâmetro no retorno de chamada de falha. Deixe-me mostrar-lhe isto. Se você for para processing_data. Digamos que você queira executar algo se essa tarefa falhar. Bem, você só precisa adicionar um terceiro parâmetro, que é on_failure_callback, e depois colocar o código que deseja acionar se essa tarefa falhar. Digamos _falha.
Então, aqui logo acima do objeto deck, criamos outra função, _failure, com o contexto, você pode pegar o contexto aqui e vamos imprimir uma mensagem, 

\&quot;On callback failed,\&quot;

e então o contexto também. Assim. Depois de configurar o retorno de chamada, modifique a saída zero para sair um, para criar uma falha falsa. Salve o arquivo, volte para a IU. Vamos selecionar esta tarefa. Clique em limpar para tentar novamente, ok, então vamos esperar um pouco.

A tarefa falhou conforme o esperado. Se você clicar nele e ir para o log, agora procure \&quot;Na falha,\&quot; ou \&quot;Na falha do retorno de chamada,\&quot; você pode ver ali, a mensagem vinda do retorno de chamada, o retorno de chamada em caso de falha, bem como a ação de contexto ali. Portanto, com a falha de retorno de chamada, você pode executar um código se uma tarefa falhar, o que é super, super útil.

##
### Ops...We got a failure
##

At this point, we have successfully created a data pipeline in order to discover all the concepts that you might have during the exam. But there is still something missing. Indeed, what if we have a task in failure? What should we do? How can we monitor this? That's what we are going to discover in that video.
So let's create a fake error. To the DAG. In processing data with the BashOperator, instead of exit with zero, we put one. And by doing this, this task will end up in failure. Save the file and let's go on the user interface. From there we start scheduling the data pipeline, refresh the page. As you can see, we have one DAG run running. Let's refresh again, and we have an issue. If we click on the DAG in graph view, as you can see, processing data is up for retry.

So what does it mean? It means that this task had a failure and so Airflow will automatically try to run it again. And in fact, if you go into the code right there in the default arguments, we have specified that each time a task has a failure, the task should be retried five times before ending up with the state use failed. And between each try, we have a time delta delay of five minutes. So between each try, we have five minutes. That's what you can see here.
So what should we do whenever we have a task in failure? Well, you should click on the task, then go to the log and from there, try to debug what's going on with your task. As you can see here, the command returned a non-zero exit code. So if we want to fix that issue, we just need to go back to the task right there and put zero instead of one, save the file, go back to the UI, click on graph view, then the task and click on clear.

Clear allows you to retry the task. Click on, okay. Now the border color is in white. And if we wait a little bit, processing data has been successfully completed. Now, what if you have multiple DAG runs with multiple failures? Let me show you this. Pause the DAG. Let's go to browse, DAG runs and delete the DAG run to clear everything. Same for the task instances. Select all of them, actions and clear.
And let's say instead of having the catchup parameters set to false, set it to true so that we will end up with three DAG runs. Finally, in the batch operator, put one in order to create a failure, save the file, go back to the UI, click on DAGs and let's start scheduling the DAG again. Refresh the page. As you can see this time, we have three DAG runs running, but we also have three tasks up for retry.

Indeed, if we click on the DAG from the tree view, which is the best view if you want to take a look at all of your DAG runs. We have three DAG runs and three tasks up for retry. Okay, here, the naive way to fix your task is to click on the task, then click on clear in order to retry your task. But if you have like hundreds of DAG runs, definitely you don't want to do that. It'll be pretty boring and time consuming. So instead of doing this, go to the browse, task instances, and then click on search, add filter and state. From there, look for the state up_for_retry, hit enter and you obtain only the tasks for which you are interested by. And if you click on search and add a second filter, you can even filter on the DAG ID.

So if you click on the DAG ID and then add simple_dag, you make sure that you will get only the task instances corresponding to the right DAG. Once you have this, go to the code, fix the issue. So let's put zero here, save the file, and then select all of the tasks, click on actions and clear. So basically here we are going to retry all of those tasks at once, instead of having to click on the task, click on clear and so on for each diagram.

So click on, okay, go back to DAGs and now it's done. That's what you can see from the tree view. So this is the easiest way to retry multiple tasks at the same time. And again, you will end up with many DAG runs and sometime with many failures at the same point. So that's why I truly advise you to use browse, task instances, as well as the filters in order to retry your tasks once you have fixed your issue.

And one more thing, avoid retrying the DAG run because if you click on the second right there and click on clear, all the tasks are going to be retried. But in fact, if only one task has failed well, again, you are wasting your resources and your time. So try to be specific whenever you have a failure and retry only the task in failure, not the whole DAG run.

All right, now I would like to show you two more things. Let's say you want to be warned if there is a failure, or if there is a retry. How can you do that? Well, there are two parameters to do this. In the default arguments, if you specify the parameter email_on_failure equals to true, you will receive an email as soon as the task is in failure. If you want to receive an email as soon as the task is retried, you can put email_on_retry with the value true and again, you'll receive an email as soon as the task is retried.

Then last but not least, you have to specify an email if you want to get those parameters working. So let's put an email like admin@astro.io, then finally, you have to configure the SMTP server, otherwise you won't be able to send any emails from your air for instance. So that's one way to be warned if there is a failure or a retry in your DAG.

Now, what else? Let's say you want to execute something if a task fails. How can you do that? By using the parameter on failure callback. Let me show you this. If you go to processing_data. Let's say you want to execute something if that task fails. Well, you just need to add a third parameter, which is on_failure_callback, and then put the code that you want to trigger if that task fails. Let's say _failure.
Then here just above the deck object, we create another function, _failure, with the context, you can get the context here and let's print a message, \&quot;On callback failure,\&quot; and then the context as well. Like that. Once you have the callback set up, modify exit zero to exit one, to create a fake failure. Save the file, go back to the UI. Let's select this task. Click on clear in order to retry it, okay, then let's wait a little bit.

The task has failed as expected. If you click on it and go to the log, then now look for, \&quot;On failure,\&quot; or \&quot;On callback failure,\&quot; you can see right there, the message coming from the callback, the on failure callback as well as the context action right there. So with the on callback failure, you can execute a code if a task fails, which is super, super useful.


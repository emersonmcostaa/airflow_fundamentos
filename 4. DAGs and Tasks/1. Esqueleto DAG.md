### Esqueleto DAG
##
É isso. É hora de mergulhar no código. Vamos criar um pipeline de dados muito simples, seguindo as melhores práticas para focar no que você pode ter durante o exame. Espero que você esteja pronto. E se estiver, abra seu editor de código na pasta Astronomer e vamos criar nosso primeiro pipeline de dados.

Na pasta DAGS, vamos criar um novo arquivo e nomeá-lo. Digamos simple_DAG.py. Assim que tiver esse arquivo, você estará pronto para criar seu pipeline de dados. Qual é a primeira linha que você tem que colocar? Bom, você tem que fazer o import da classe DAG. Vamos digitar do airflow import DAG. A classe DAG será usada para instanciar um objeto DAG. E assim crie um pipeline de dados no fluxo de ar. É por isso que sempre temos que fazer essa importação primeiro.

Em seguida, duas linhas abaixo. Estamos prontos para instanciar um objeto DAG. Como podemos fazer isso? Bem, uma maneira bastante comum de definir um objeto DAG é criando uma variável, assim, igual a DAG, então blá, blá, blá. E então crie algumas tarefas, como a tarefa um igual ao operador. Então aqui você diz que esse operador pertence ao DAG. O mesmo para a tarefa dois, assim. E assim por diante. Bem, na verdade, existe uma maneira melhor e mais limpa de instanciar um objeto DAG. Esta é a declaração [inaudível 00:01:38]. Em vez de ter todo esse código, uma coisa que podemos fazer é ter um with, a instrução with, depois o objeto DAG, então aqui vamos colocar alguns parâmetros como DAG, dois pontos. E aí podemos colocar nossas tarefas sem precisar falar, esse operador pertence ao DAG com DAG igual a DAG. Não precisamos fazer isso.

Podemos apenas digitar o sublinhado da tarefa um igual ao operador. E é isso. Não há necessidade de colocar DAG igual a DAG. Isso torna seu deck mais limpo e você pode ver claramente que todas as suas tarefas pertencem aos seus DAGs, pois estarão sob a instrução with.

Depois de instanciar o objeto DAG, o que vem a seguir? Bom, temos que definir o primeiro e mais importante parâmetro, eu diria, que é o DAG ID. O DAG ID é o identificador exclusivo do seu DAG e você deve garantir que ele seja exclusivo em todos os seus DAGs. Caso contrário, você pode acabar com comportamentos estranhos em que o DAG mais recente é levado em consideração. Você poderá vê-lo na interface do usuário, mas não na mais antiga. Certifique-se de que seus IDs DAG sejam sempre exclusivos. Nesse caso, podemos colocar o ID de sublinhado DAG igual ao DAG de sublinhado simples. Normalmente, seu DAG ID será o nome final do seu DAG.

Tudo bem, agora temos isso. O que devemos fazer em seguida? Mas, na verdade, nada. Não há mais nada a fazer. Neste ponto. Você criou com sucesso um pipeline de dados no airflow. Fizemos a importação para a classe DAG. Em seguida, instanciamos um objeto DAG. Obviamente, isso não funciona porque o operador não existe, então vamos removê-lo. E se você não colocar nenhum assim, é a única coisa que você precisa para criar um pipeline de dados no fluxo de ar. Vamos verificar isso na interface do usuário. Salve o arquivo e vá para a interface do usuário do fluxo de ar. Digite admin, admin. Aperte enter. E como você pode ver, obtemos o DAG simples na partitura DAG. Esse é o DAG que acabamos de criar.

Há algumas coisas a serem observadas aqui. Primeiro, não há proprietário por padrão. É o que podemos ver aqui. E o agendamento [inaudível 00:03:56] está definido para um dia. Por padrão, seus DAGs são programados para serem executados a cada 24 horas. Esse é o intervalo agendado padrão. Isso é ótimo. Sabemos que, por padrão, esse DAG deve ser acionado a cada 24 horas.

Mas e a primeira data em que começará a ser agendada? Digamos que queremos começar a agendar esse pipeline de dados em 1º de janeiro de 2021. E, em seguida, acionar esse DAG a cada 24 horas, conforme definido por padrão. Obviamente há algo faltando aqui, que é a data de início. Sempre que você cria um DAG no airflow, há dois parâmetros que você deve definir. A primeira é a data de início. E o segundo é o intervalo de agendamento. A data de início e o intervalo de agendamento definem quando e com que frequência seu DAG deve ser acionado. Como esse agendamento pode ser bem difícil no começo, vamos desmistificar no próximo vídeo, pois é super importante entender.


##
### DAG Skeleton
##
This is it. It's time to dive into the code. We are going to create a very simple data pipeline, following best practices in order to focus on what you might have during the exam. I hope you're ready. And if you are, well open your code editor in the folder, Astronomer, and let's create our first data pipeline.

In the folder, DAGS, let's create a new file and name it. Let's say simple_DAG.py. Once you have this file, you are ready to create your data pipeline. What is the first line you have to put? Well, you have to make the import of the DAG class. Let's type from airflow import DAG. The DAG class will be used in order to instantiate a DAG object. And so create a data pipeline in airflow. That's why we always have to make this import first.

Then two lines below. We are ready to instantiate a DAG object. How can we do that? Well, a pretty common way of defining a DAG object is by creating a variable, like that, equals to DAG, then blah, blah, blah. And then create some tasks, like task one equals to operator. Then here you say that this operator belongs to the DAG. Same for task two, like that. And so on. Well, actually there is a better way, and a cleaner way to instantiate a DAG object. This way is the [inaudible 00:01:38] statement. Instead of having all of this code, one thing we can is to have a with, the with statement, then the DAG object, then here we're going to put some parameters as DAG, colon. And then here we can put our tasks without having to say, this operator belongs to the DAG with DAG equal to DAG. We don't have to do that.

We can just type task underscore one equals to operator. And that's it. No need to put DAG equals DAG. It makes your deck cleaner, and you can clearly see that all of your tasks belong to your DAGs, as they will be under the with statement.

Once we have instantiated the DAG object, what next? Well, we have to define the first and most important parameter, I would say, which is the DAG ID. The DAG ID is the unique identifier of your DAG, and you have to make sure it is unique across all of your DAGs. Otherwise you might end up with weird behaviors where the most recent DAG is taken into account. You will be able to see it on UI, but not the older one. Make sure that your DAG IDs are always unique. In that case, we can put DAG underscore ID equals to simple underscore DAG. Usually your DAG ID will be the final name of your DAG.

All right, now we have that. What should we do next? But actually, nothing. There is nothing else to do. At this point. You have successfully created a data pipeline in airflow. We have made the import for the DAG class. Then we have in instantiated a DAG object. Obviously this doesn't work as operator doesn't exist, so let's remove it. And if you put none like that, that's the only thing you need in order to create a data pipeline in airflow. Let's verify this on the user interface. Save the file, then go to the UI of airflow. Type admin, admin. Hit enter. And as you can see, we obtain the DAG simple on the score DAG. That's the DAG we have just created.

There are a few things to notice here. First, there is no owner by default. That's what we can see here. And the scheduling [inaudible 00:03:56] is set to one day. By default, your DAGs are scheduled to run every 24 hours. That's the default scheduled interval. That's great. We know that by default, this DAG should be triggered every 24 hours.

But what about the first date at which it will start being scheduled? Let's say we want to start scheduling this data pipeline the 1st of January, 2021. And then trigger that DAG every 24 hours, as defined by default. Obviously there is something missing here, which is the start date. Whenever you create a DAG in airflow, there are two parameters that you have to define. The first one is the start date. And the second one is the schedule interval. The start date, and the schedule interval define when, and how often your DAG should be triggered. As that scheduling can be pretty difficult at first, lets demystify it in the next video, as it is super important to understand.